{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ed81a29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/export/home/phys/kyungmip/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:80: UserWarning: CUDA initialization: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 803: system has unsupported display driver / cuda driver combination (Triggered internally at  ../c10/cuda/CUDAFunctions.cpp:112.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import json\n",
    "import os.path as osp\n",
    "import os,sys\n",
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.utils import to_undirected\n",
    "from torch_cluster import radius_graph, knn_graph\n",
    "from torch_geometric.datasets import MNISTSuperpixels\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "from time import strftime, gmtime\n",
    "\n",
    "sys.path.append('../')\n",
    "import utils\n",
    "import model.net as net\n",
    "import model.data_loader as data_loader\n",
    "from evaluate import evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517341d9",
   "metadata": {},
   "source": [
    "### Change from DeepMETv2\n",
    "\n",
    "1. Change x_cont, x_cat, etaphi in train() in accordance with the change of training inputs \n",
    "2. Add n_features_cont, n_features_cat to keep track of these numbers here and there, i.e. when building a model these numbers go into arguments\n",
    "3. Remove the resolution-MET plotting part in evaluate, as L1 doesn't have bunch of METs that DeepMETv2 has access to\n",
    "4. Add input scaling to [0,1] (norm)\n",
    "5. Add weight_decay to the optimizer, remove patience from the scheduler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf7c93ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "doRestore = False\n",
    "restore_file = 'best'      # Optional, name of the file in --model_dir containing weights to reload before training\n",
    "\n",
    "pre_fix = '/export/home/phys/kyungmip/L1DeepMETv2/'\n",
    "\n",
    "data_dir = pre_fix + 'data_ttbar/'        # name of the input data folder\n",
    "ckpts = pre_fix + 'ckpts_Mar11_scaled/'          # name of the output ckpts folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ccba2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "batch_size = 32\n",
    "lr = 1e-4\n",
    "weight_decay = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5dfe84ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "removePuppi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19a90e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features_cont = 6\n",
    "\n",
    "n_features_cat = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26098c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_momentum = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91ab90e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, optimizer, scheduler, loss_fn, dataloader, removePuppi = False):\n",
    "    model.train()\n",
    "    \n",
    "    loss_avg_arr = []\n",
    "    loss_avg = utils.RunningAverage()\n",
    "\n",
    "    with tqdm(total=len(dataloader)) as t:\n",
    "        for data in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            data = data.to(device)\n",
    "            \n",
    "            if removePuppi:\n",
    "                x_cont = data.x[:,:(n_features_cont-1)]\n",
    "            else:\n",
    "                x_cont = data.x[:,:n_features_cont]       # include puppi\n",
    "            \n",
    "            x_cat = data.x[:,n_features_cont:].long()\n",
    "\n",
    "            #phi = torch.atan2(data.x[:,2], data.x[:,1])   # atan2(py, px)\n",
    "            etaphi = torch.cat([data.x[:,3][:,None], data.x[:,4][:,None]], dim=1)\n",
    "\n",
    "            # NB: there is a problem right now for comparing hits at the +/- pi boundary\n",
    "            edge_index = radius_graph(etaphi, r=deltaR, batch=data.batch, loop=False, max_num_neighbors=255)  # turn off self-loop\n",
    "            result = model(x_cont, x_cat, edge_index, data.batch)\n",
    "            \n",
    "            # sample_weight = torch.zeros((data.y).shape[0]).to(device) # number of nodes\n",
    "    \n",
    "            loss = loss_fn(result, data.x, data.y, data.batch, scale_momentum)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # update the average loss\n",
    "            loss_avg_arr.append(loss.item())\n",
    "            \n",
    "            loss_avg.update(loss.item())\n",
    "            t.set_postfix(loss='{:05.3f}'.format(loss_avg()))\n",
    "            t.update()\n",
    "    \n",
    "    scheduler.step(np.mean(loss_avg_arr))\n",
    "    print('Training epoch: {:02d}, MSE: {:.4f}'.format(epoch, np.mean(loss_avg_arr)))\n",
    "\n",
    "    model_dir = osp.join(ckpts)\n",
    "    os.system('mkdir -p {}/MODELS'.format(model_dir))\n",
    "\n",
    "    return np.mean(loss_avg_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f26a99a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/export/home/phys/kyungmip'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ['PWD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6fb05a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split: 199708\n",
      "length of train/val data:  798834 199708\n",
      "Training dataloader: 24964, Test dataloader: 6241\n"
     ]
    }
   ],
   "source": [
    "dataloaders = data_loader.fetch_dataloader(data_dir = data_dir, batch_size=int(batch_size), validation_split=.2)\n",
    "\n",
    "train_dl = dataloaders['train']\n",
    "test_dl = dataloaders['test']\n",
    "\n",
    "print('Training dataloader: {}, Test dataloader: {}'.format(len(train_dl), len(test_dl)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb600371",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(0)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "566c5efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA_LAUNCH_BLOCKING=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "abd4ac1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input norm\n",
    "if removePuppi:\n",
    "    norm = torch.tensor([1./scale_momentum, 1./scale_momentum, 1./scale_momentum, 1., 1.]).to(device)\n",
    "else:\n",
    "    norm = torch.tensor([1./scale_momentum, 1./scale_momentum, 1./scale_momentum, 1., 1., 1.]).to(device)\n",
    "\n",
    "#norm = torch.tensor([1./499.25, 1./491.312, 1./495.928, 1./5.035, 1./3.142, 1.]).to(device)   # Have inputs within [0,1]\n",
    "\n",
    "# Model\n",
    "if removePuppi:\n",
    "    model = net.Net((n_features_cont-1), n_features_cat, norm).to(device)\n",
    "else:\n",
    "    model = net.Net(n_features_cont, n_features_cat, norm).to(device)\n",
    "\n",
    "# Optimizer and scheduler\n",
    "optimizer = torch.optim.AdamW(model.parameters(),lr=float(lr), weight_decay=float(weight_decay))\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr = 1e-5, max_lr = 1e-4, cycle_momentum=False)\n",
    "#scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr = 0.1)\n",
    "#scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, threshold=0.05)\n",
    "#scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=500, threshold=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af00e107",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_epoch = 0\n",
    "best_validation_loss = 10e7\n",
    "    \n",
    "deltaR = 0.4\n",
    "deltaR_dz = 0.3 # not used\n",
    "\n",
    "#loss_fn = net.loss_fn\n",
    "loss_fn = net.loss_fn_response_tune\n",
    "\n",
    "metrics = net.metrics\n",
    "\n",
    "model_dir = ckpts\n",
    "\n",
    "os.system('mkdir -p {}'.format(model_dir))\n",
    "loss_log = open(model_dir+'/loss.log', 'w')\n",
    "loss_log.write('# loss log for training starting in '+strftime(\"%Y-%m-%d %H:%M:%S\", gmtime()) + '\\n')\n",
    "loss_log.write('epoch, loss, val_loss\\n')\n",
    "loss_log.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "deaf799d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload weights from restore_file if specified\n",
    "if doRestore:\n",
    "    if restore_file is not None:\n",
    "        restore_ckpt = osp.join(model_dir, restore_file + '.pth.tar')\n",
    "        ckpt = utils.load_checkpoint(restore_ckpt, model, optimizer, scheduler)\n",
    "        first_epoch = ckpt['epoch']\n",
    "        print('Restarting training from epoch',first_epoch)\n",
    "        with open(osp.join(model_dir, 'metrics_val_best.json')) as restore_metrics:\n",
    "            best_validation_loss = json.load(restore_metrics)['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6ddba41b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/24964 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current best loss: 100000000.0\n",
      "Learning rate: 1e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 13753/24964 [22:34<18:24, 10.15it/s, loss=6885.011]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-89cea6b845c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# compute number of batches in one epoch (one full pass over the training set)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# Save weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-45446affc3b4>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, device, optimizer, scheduler, loss_fn, dataloader, removePuppi)\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_momentum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(first_epoch+1, epochs):\n",
    "    print('Current best loss:', best_validation_loss)\n",
    "    if '_last_lr' in scheduler.state_dict():\n",
    "        print('Learning rate:', scheduler.state_dict()['_last_lr'][0])\n",
    "\n",
    "    # compute number of batches in one epoch (one full pass over the training set)\n",
    "    train_loss = train(model, device, optimizer, scheduler, loss_fn, train_dl)\n",
    "    \n",
    "    # Save weights\n",
    "    utils.save_checkpoint({'epoch': epoch,\n",
    "                            'state_dict': model.state_dict(),\n",
    "                            'optim_dict': optimizer.state_dict(),\n",
    "                            'sched_dict': scheduler.state_dict()},\n",
    "                            is_best=False,\n",
    "                            checkpoint=model_dir)\n",
    "\n",
    "    # save model\n",
    "    # m = torch.jit.script(model)\n",
    "    # torch.jit.save(m, f'{model_dir}/MODELS/scripted_model_epoch{epoch}.pt')\n",
    "    # torch.save(model, f'{model_dir}/MODELS/model_epoch{epoch}.pt')\n",
    "    \n",
    "    # Evaluate for one epoch on validation set\n",
    "    test_metrics, resolutions, MET_arr = evaluate(model, device, loss_fn, test_dl, metrics, deltaR, deltaR_dz, model_dir, epoch, n_features_cont, save_METarr = True, removePuppi = removePuppi)\n",
    "\n",
    "    validation_loss = test_metrics['loss']\n",
    "    loss_log.write('%d,%.8f,%.8f\\n'%(epoch, train_loss, validation_loss))\n",
    "    loss_log.flush()\n",
    "    is_best = (validation_loss<=best_validation_loss)\n",
    "\n",
    "    # If best_eval, best_save_path\n",
    "    if is_best: \n",
    "        print('Found new best loss!') \n",
    "        best_validation_loss=validation_loss\n",
    "\n",
    "        # Save weights\n",
    "        utils.save_checkpoint({'epoch': epoch,\n",
    "                                'state_dict': model.state_dict(),\n",
    "                                'optim_dict': optimizer.state_dict(),\n",
    "                                'sched_dict': scheduler.state_dict()},\n",
    "                                is_best=True,\n",
    "                                checkpoint=model_dir)\n",
    "            \n",
    "        # Save best val metrics in a json file in the model directory\n",
    "        utils.save_dict_to_json(test_metrics, osp.join(model_dir, 'metrics_val_best.json'))\n",
    "        utils.save(resolutions, osp.join(model_dir, 'best.resolutions'))\n",
    "\n",
    "    utils.save_dict_to_json(test_metrics, osp.join(model_dir, 'metrics_val_last.json'))\n",
    "    utils.save(resolutions, osp.join(model_dir, 'last.resolutions'))\n",
    "\n",
    "loss_log.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0147635",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfce245b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
