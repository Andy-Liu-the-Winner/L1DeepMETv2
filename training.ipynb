{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ed81a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import json\n",
    "import os.path as osp\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.utils import to_undirected\n",
    "from torch_cluster import radius_graph, knn_graph\n",
    "from torch_geometric.datasets import MNISTSuperpixels\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "import utils\n",
    "import model.net as net\n",
    "import model.data_loader as data_loader\n",
    "from evaluate import evaluate\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "from time import strftime, gmtime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517341d9",
   "metadata": {},
   "source": [
    "### Change from DeepMETv2\n",
    "\n",
    "1. Change x_cont, x_cat, etaphi in train() in accordance with the change of training inputs \n",
    "2. Add n_features_cont, n_features_cat to keep track of these numbers here and there, i.e. when building a model these numbers go into arguments\n",
    "3. Remove the resolution-MET plotting part in evaluate, as L1 doesn't have bunch of METs that DeepMETv2 has access to\n",
    "4. Add input scaling to [0,1] (norm)\n",
    "5. Add weight_decay to the optimizer, remove patience from the scheduler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf7c93ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "doRestore = False\n",
    "restore_file = 'best'      # Optional, name of the file in --model_dir containing weights to reload before training\n",
    "\n",
    "pre_fix = '/export/home/phys/kyungmip/L1DeepMETv2/'\n",
    "\n",
    "data_dir = pre_fix + 'data_ttbar/'        # name of the input data folder\n",
    "ckpts = pre_fix + 'ckpts_Feb18/'          # name of the output ckpts folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3ccba2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "batch_size = 32\n",
    "lr = 1e-4\n",
    "weight_decay = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19a90e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features_cont = 6\n",
    "n_features_cat = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91ab90e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, optimizer, scheduler, loss_fn, dataloader):\n",
    "    model.train()\n",
    "    \n",
    "    loss_avg_arr = []\n",
    "    loss_avg = utils.RunningAverage()\n",
    "\n",
    "    with tqdm(total=len(dataloader)) as t:\n",
    "        for data in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            data = data.to(device)\n",
    "\n",
    "            x_cont = data.x[:,:n_features_cont]       # include puppi\n",
    "            #x_cont = data.x[:,:(n_features_cont-1)]  # remove puppi\n",
    "            x_cat = data.x[:,n_features_cont:].long()\n",
    "\n",
    "            #phi = torch.atan2(data.x[:,2], data.x[:,1])   # atan2(py, px)\n",
    "            etaphi = torch.cat([data.x[:,3][:,None], data.x[:,4][:,None]], dim=1)\n",
    "\n",
    "            # NB: there is a problem right now for comparing hits at the +/- pi boundary\n",
    "            edge_index = radius_graph(etaphi, r=deltaR, batch=data.batch, loop=False, max_num_neighbors=255)  # turn off self-loop\n",
    "            result = model(x_cont, x_cat, edge_index, data.batch)\n",
    "            \n",
    "            loss = loss_fn(result, data.x, data.y, data.batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # update the average loss\n",
    "            loss_avg_arr.append(loss.item())\n",
    "            \n",
    "            loss_avg.update(loss.item())\n",
    "            t.set_postfix(loss='{:05.3f}'.format(loss_avg()))\n",
    "            t.update()\n",
    "    \n",
    "    scheduler.step(np.mean(loss_avg_arr))\n",
    "    print('Training epoch: {:02d}, MSE: {:.4f}'.format(epoch, np.mean(loss_avg_arr)))\n",
    "\n",
    "    model_dir = osp.join(ckpts)\n",
    "    os.system('mkdir -p {}/MODELS'.format(model_dir))\n",
    "\n",
    "    return np.mean(loss_avg_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f26a99a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/export/home/phys/kyungmip'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ['PWD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6fb05a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split: 199708\n",
      "length of train/val data:  798834 199708\n",
      "Training dataloader: 24964, Test dataloader: 6241\n"
     ]
    }
   ],
   "source": [
    "dataloaders = data_loader.fetch_dataloader(data_dir = data_dir, batch_size=int(batch_size), validation_split=.2)\n",
    "\n",
    "train_dl = dataloaders['train']\n",
    "test_dl = dataloaders['test']\n",
    "\n",
    "print('Training dataloader: {}, Test dataloader: {}'.format(len(train_dl), len(test_dl)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb600371",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(2)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "abd4ac1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input norm\n",
    "norm = torch.tensor([1., 1., 1., 1., 1., 1.]).to(device)\n",
    "\n",
    "#norm = torch.tensor([1./499.25, 1./491.312, 1./495.928, 1./5.035, 1./3.142, 1.]).to(device)   # Have inputs within [0,1]\n",
    "\n",
    "# Model\n",
    "model = net.Net(n_features_cont, n_features_cat, norm).to(device) #include puppi\n",
    "#model = net.Net(n_features_cont-1, n_features_cat, norm).to(device) #remove puppi\n",
    "\n",
    "# Optimizer and scheduler\n",
    "optimizer = torch.optim.AdamW(model.parameters(),lr=float(lr), weight_decay=float(weight_decay))\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr = 1e-5, max_lr = 1e-4, cycle_momentum=False)\n",
    "#scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr = 0.1)\n",
    "#scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, threshold=0.05)\n",
    "#scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=500, threshold=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "af00e107",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_epoch = 0\n",
    "best_validation_loss = 10e7\n",
    "    \n",
    "deltaR = 0.4\n",
    "deltaR_dz = 0.3 # not used\n",
    "\n",
    "#loss_fn = net.loss_fn\n",
    "loss_fn = net.loss_fn_response_tune\n",
    "metrics = net.metrics\n",
    "\n",
    "model_dir = ckpts\n",
    "\n",
    "os.system('mkdir -p {}'.format(model_dir))\n",
    "loss_log = open(model_dir+'/loss.log', 'w')\n",
    "loss_log.write('# loss log for training starting in '+strftime(\"%Y-%m-%d %H:%M:%S\", gmtime()) + '\\n')\n",
    "loss_log.write('epoch, loss, val_loss\\n')\n",
    "loss_log.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "deaf799d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload weights from restore_file if specified\n",
    "if doRestore:\n",
    "    if restore_file is not None:\n",
    "        restore_ckpt = osp.join(model_dir, restore_file + '.pth.tar')\n",
    "        ckpt = utils.load_checkpoint(restore_ckpt, model, optimizer, scheduler)\n",
    "        first_epoch = ckpt['epoch']\n",
    "        print('Restarting training from epoch',first_epoch)\n",
    "        with open(osp.join(model_dir, 'metrics_val_best.json')) as restore_metrics:\n",
    "            best_validation_loss = json.load(restore_metrics)['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddba41b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/24964 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current best loss: 38021.770460400476\n",
      "Learning rate: 8.991181673289238e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|â–Ž         | 927/24964 [00:33<17:07, 23.38it/s, loss=37772.351]"
     ]
    }
   ],
   "source": [
    "for epoch in range(first_epoch+1, epochs):\n",
    "    print('Current best loss:', best_validation_loss)\n",
    "    if '_last_lr' in scheduler.state_dict():\n",
    "        print('Learning rate:', scheduler.state_dict()['_last_lr'][0])\n",
    "\n",
    "    # compute number of batches in one epoch (one full pass over the training set)\n",
    "    train_loss = train(model, device, optimizer, scheduler, loss_fn, train_dl)\n",
    "    \n",
    "    # Save weights\n",
    "    utils.save_checkpoint({'epoch': epoch,\n",
    "                            'state_dict': model.state_dict(),\n",
    "                            'optim_dict': optimizer.state_dict(),\n",
    "                            'sched_dict': scheduler.state_dict()},\n",
    "                            is_best=False,\n",
    "                            checkpoint=model_dir)\n",
    "\n",
    "    # save model\n",
    "    # m = torch.jit.script(model)\n",
    "    # torch.jit.save(m, f'{model_dir}/MODELS/scripted_model_epoch{epoch}.pt')\n",
    "    # torch.save(model, f'{model_dir}/MODELS/model_epoch{epoch}.pt')\n",
    "    \n",
    "    # Evaluate for one epoch on validation set\n",
    "    test_metrics, resolutions, MET_arr = evaluate(model, device, loss_fn, test_dl, metrics, deltaR, deltaR_dz, model_dir, epoch, save_METarr = True)\n",
    "\n",
    "    validation_loss = test_metrics['loss']\n",
    "    loss_log.write('%d,%.8f,%.8f\\n'%(epoch, train_loss, validation_loss))\n",
    "    loss_log.flush()\n",
    "    is_best = (validation_loss<=best_validation_loss)\n",
    "\n",
    "    # If best_eval, best_save_path\n",
    "    if is_best: \n",
    "        print('Found new best loss!') \n",
    "        best_validation_loss=validation_loss\n",
    "\n",
    "        # Save weights\n",
    "        utils.save_checkpoint({'epoch': epoch,\n",
    "                                'state_dict': model.state_dict(),\n",
    "                                'optim_dict': optimizer.state_dict(),\n",
    "                                'sched_dict': scheduler.state_dict()},\n",
    "                                is_best=True,\n",
    "                                checkpoint=model_dir)\n",
    "            \n",
    "        # Save best val metrics in a json file in the model directory\n",
    "        utils.save_dict_to_json(test_metrics, osp.join(model_dir, 'metrics_val_best.json'))\n",
    "        utils.save(resolutions, osp.join(model_dir, 'best.resolutions'))\n",
    "\n",
    "    utils.save_dict_to_json(test_metrics, osp.join(model_dir, 'metrics_val_last.json'))\n",
    "    utils.save(resolutions, osp.join(model_dir, 'last.resolutions'))\n",
    "\n",
    "loss_log.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87806a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
